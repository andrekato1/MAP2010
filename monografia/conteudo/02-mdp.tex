%!TeX root=../tese.tex
%("dica" para o editor de texto: este arquivo Ã© parte de um documento maior)
% para saber mais: https://tex.stackexchange.com/q/78101

\chapter{Markov Decision Processes}

\section{Markov Processes}

In the reinforcement learning framework, the agent makes decisions as a function of a signal from the environment called the \textbf{state}. In this section, we discuss what is required of the state signal and what information it does or does not convey.

The state signal should include an immediate sensation, but it could include more than that, including some memory from past states. In fact, in typical applications, it usually is expected the state to inform the agent of more than just immediate sensations. For example, we could hear the word "yes", but we could be in totally different states depending on what was the question that came before and can no longer be heard. In contrast, the state signal should not be expected to inform the agent about all of the past experiences ou all about the environment. Ideally, we want the state signal to summarize well past experiences, in such a way all the relevant information is retained. 

To formalize this idea, for simplicity, suppose there are a finite number of states and rewards. Also, consider that the environment responds at time $t+1$ to a action taken at time $t$. We define the history sequence as $h_t = \{S_0, A_0, R_1, \dots, S_{t-1}, A_{t-1}, R_t, S_t, A_t\}$. Assume the state is a function of the history, that is, $S_t = f(h_t)$. If the state signal does not have the Markov property, the response of the environment depends on everything that happened earlier. Otherwise, the environment's response depends only on the state and actions at time $t$.
\begin{definition}{Markov Property}{markov_property}
      A state signal is said to have the Markov property if and only if
      \begin{equation}\label{eq:markov_property}
            P\{R_{t+1} = r, S_{t+1} = s' \mid h_t\} = P\{R_{t+1} = r, S_{t+1} = s' \mid S_t, A_t\}
      \end{equation}
      for all $r$, $s'$ and $h_t$. If \ref{eq:markov_property} is satisfied, then the environment also has the Markov property.   
\end{definition}
If every state of an environment is Markov, then we define a Markov Decision Process (MDP) as follows:
\begin{definition}{Markov Decision Process}{mdp}
      A \textit{Markov Decision Process} is a tuple $\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$
      \begin{itemize}
            \item $\mathcal{S}$ is a finite set of states
            \item $\mathcal{A}$ is a finite set of actions
            \item $\mathcal{P}$ is a state transition probability matrix \\$\mathcal{P}^a_{ss'} = P[S_{t+1} = s' \mid S_t = s, A_t = a]$
            \item $\mathcal{R}$ is a reward function\\$\mathcal{R}^a_s = \mathbb{E}[R_{t+1} \mid S_t = s, A_t = a]$
            \item $\gamma$ is a discount factor such that $\gamma \in [0, 1]$
      \end{itemize}  
\end{definition}

\section{Policies}

A big part of reinforcement learning is estimating how much reward the agent is expected to get by being in a specific state. In order to do that, we introduce the \textit{value function} - a function of a state or a state-action pair that estimates the expected future rewards (or expected return), which tells us how good it is to be in a certain state or how good it is to take a specific action while in a specific state. Accordingly, value functions are defined with respect to different ways of acting and these ways of acting are dictated by a \textit{policy}. 

A policy is a mapping of actions to states, defining a probability distribution over $a \in \mathcal{A}$ for each $s \in \mathcal{S}$.
\begin{definition}{Policy}{policy}
      A \textit{policy} $\pi$ is a probability distribution over actions given states,
      \begin{equation}\label{eq:policy_def}
            \pi(a \mid s) = p[A_t = a \mid  S_t = s]
      \end{equation}  
\end{definition}
A policy fully defines the behavior of and agent, and is stationary, that is, $A_t \sim \pi(\cdot \mid S_t)$, for all $t > 0$.

Given an MDP $\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$ and a policy $\pi$, we define the value function:
\begin{definition}{State-Value Function}{statevalue_func}
      The \textit{state-value function} $v_\pi(s)$ of an MDP is the expected return starting from state $s$ and following policy $\pi$ afterwards
      \begin{equation}\label{eq:statevalue_func_def}
            v_\pi(s) = \mathbb{E}_\pi[G_t \mid S_t = s] = \mathbb{E}_\pi\left[\sum^{\infty}_{k=0}\gamma^kR_{t+k+1} \mid S_t = s\right]
      \end{equation}  
\end{definition}

\begin{definition}{Action-Value Function}{actionvalue_func}
      The \textit{action-value function} $q_\pi(s, a)$ of an MDP is the expected return starting from state $s$, taking action $a$ and following policy $\pi$ afterwards
      \begin{equation}\label{eq:actionvalue_func_def}
            q_\pi(s, a) = \mathbb{E}_\pi[G_t \mid S_t = s, A_t = a] = \mathbb{E}_\pi\left[\sum^{\infty}_{k=0}\gamma^kR_{t+k+1}\mid S_t = s, A_t = a\right]
      \end{equation}  
\end{definition}
The value function $v_\pi$ can be estimated using \textit{Monte Carlo Methods} by keeping the average of the returns that followed each state and then the average will eventually converge to the state's true value as the number of times that state is encountered approaches infinity. Similarly, $q_\pi$ can be estimated by the same method by keeping the average of each state and each action taken in that state.

\section{Optimality}
Solving a reinforcement learning problem often means finding a policy that maximizes reward over the long run. Value functions define a partial ordering over policies:
\[
\pi \geq \pi' \text{ if } v_\pi(s) \geq v_\pi'(s) \text{ for all } s \in \mathcal{S}      
\]
\begin{theorem}{Policy Optimality}{policy_optimality}
      For any MDP, there exists an optimal policy $\pi_*$ such that $\pi_* \geq \pi$, $\forall \pi$. All optimal policies achieve the optimal value function $v_{\pi_*}$ and the optimal action-value function $v_{\pi_*}$.  
\end{theorem}
The optimal state-value function is often denoted simply as $v_*$, and is defined as 
\[
v_*(s) = \max_\pi v_\pi(s)      
\]
for all $s \in \mathcal{S}$.

Similarly, the optimal action-value function is defined as
\[
      q_*(s, a) = \max_\pi q_\pi(s, a)      
\]
for all $s \in \mathcal{S}$ and $a \in \mathcal{A}$.

There are various methods to find such optimal functions, but in this work, we'll focus on a particular class of methods called \textit{Policy Gradient Methods}. Prior to that, we'll introduce some theory on \textit{Neural Networks}.
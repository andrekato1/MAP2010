\chapter{Proximal Policy Optimization}
Proximal policy optimization (PPO) was proposed by \cite{https://doi.org/10.48550/arxiv.1707.06347} as an alternative to already existing policy gradient (PG) methods, incorporating some concepts from trust region policy optimization (TRPO) methods, retaining some of its benefits while being significantly easier to implement.
\section{Policy Gradient}
For policy gradient, we consider parameterized policies, which can select actions without relying on a value function. The value function is still useful to learn the policy parameters, but it's not strictly necessary to select an action. This parameterization can be done in any way as long as the policy is differentiable with respect to its parameters.

Denoting by $\boldsymbol\theta \in \mathbb{R}^d$ the policy parameter vector, the probability of selecting action $a$ at time $t$ given that the environment is in state $s$ with parameter $\boldsymbol\theta$ is
\[
    \pi_{\boldsymbol\theta}(a \mid s, \boldsymbol\theta) = P\{A_t = a \mid S_t = s, \boldsymbol\theta_t = \boldsymbol\theta\}   
\]
We also need to define a performance measure $J(\boldsymbol\theta)$ to quantify how good a policy is. Policy gradient algorithms search for a local maximum in $J$ using gradient ascent:
\[
    \Delta\boldsymbol\theta = \alpha\nabla_{\boldsymbol\theta}J(\boldsymbol\theta)   
\]
where $\nabla_{\boldsymbol\theta}J(\boldsymbol\theta)$ is the policy gradient defined as
\begin{align*}
    \nabla_{\boldsymbol\theta}J(\boldsymbol\theta) &=
    \begin{bmatrix}
        \frac{\partial J(\boldsymbol\theta)}{\partial \theta_1} \\
        \vdots \\
        \frac{\partial J(\boldsymbol\theta)}{\partial \theta_n}
    \end{bmatrix}
\end{align*} 
and $\alpha$ is a step size parameter, commonly called \textit{learning rate}.    

In our case, we define such measure to be $J(\boldsymbol\theta) = v_{\pi{\boldsymbol\theta}}(s_0)$, where $s_0$ denotes the starting state and $v_{\pi_{\boldsymbol\theta}}$ is the true value function for the parameterized policy $\pi_{\boldsymbol\theta}$. That is, the measure is the expected sum of discounted rewards.

For that particular choice of $J(\boldsymbol\theta)$, we have a convenient way of expressing its gradient using the Policy Gradient Theorem (adapted from \cite{NIPS1999_464d828b}):
\begin{theorem}{Policy Gradient Theorem}{pg_theorem}
    Given an MDP $\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$ and a parameterized policy $\pi_{\boldsymbol\theta}$, the gradient of the expected return $J(\boldsymbol\theta)$ is given by
    \begin{equation}
        \nabla_{\boldsymbol\theta} J(\boldsymbol\theta) \propto \sum_{s \in \mathcal{S}} d^{\pi_{\boldsymbol\theta}}(s) \sum_{a \in \mathcal{A}} q_{\pi_{\boldsymbol\theta}}(s, a) \nabla_{\boldsymbol\theta}\pi_{\boldsymbol\theta}(a \mid s)
    \end{equation}
\end{theorem}
Where $d^{\pi_{\boldsymbol\theta}}(s) = \lim_{t \to \infty} P\{s_t = s \mid s_0, \pi\}$ is the stationary distribution of states under policy $\pi$, which is assumed to be independent of the starting state $s_0$. Furthermore, we can use the following identity, commonly called the "log derivative trick", to rewrite the expression for the gradient:
\begin{align*}
    \nabla_{\boldsymbol\theta}\pi_{\boldsymbol\theta}(a \mid s) &= \pi_{\boldsymbol\theta}(a \mid s)\frac{\nabla_{\boldsymbol\theta}\pi_{\boldsymbol\theta}(a \mid s)}{\pi_{\boldsymbol\theta}(a \mid s)} \\ 
     &= \pi_{\boldsymbol\theta}(a \mid s)\nabla_{\boldsymbol\theta}\log{\pi_{\boldsymbol\theta}(a \mid s)}
\end{align*}
Then, using the identity above, we are able to express the gradient as an expectation:
\begin{align}
    \nabla_{\boldsymbol\theta} J(\boldsymbol\theta) &\propto \sum_{s \in \mathcal{S}} d^{\pi_{\boldsymbol\theta}}(s) \sum_{a \in \mathcal{A}} q_{\pi_{\boldsymbol\theta}}(s, a) \nabla_{\boldsymbol\theta}\pi_{\boldsymbol\theta}(a \mid s)\\
    &= \sum_{s \in \mathcal{S}} d^{\pi_{\boldsymbol\theta}}(s) \sum_{a \in \mathcal{A}} q_{\pi_{\boldsymbol\theta}}(s, a) \nabla_{\boldsymbol\theta}\pi_{\boldsymbol\theta}(a \mid s)\frac{\pi_{\boldsymbol\theta}(a \mid s)}{\pi_{\boldsymbol\theta}(a \mid s)}\nonumber\\
    &= \sum_{s \in \mathcal{S}} d^{\pi_{\boldsymbol\theta}}(s) \sum_{a \in \mathcal{A}} q_{\pi_{\boldsymbol\theta}}(s, a) \pi_{\boldsymbol\theta}(a \mid s)\frac{\nabla_{\boldsymbol\theta}\pi_{\boldsymbol\theta}(a \mid s)}{\pi_{\boldsymbol\theta}(a \mid s)}\nonumber\\
    &= \sum_{s \in \mathcal{S}} d^{\pi_{\boldsymbol\theta}}(s) \sum_{a \in \mathcal{A}} q_{\pi_{\boldsymbol\theta}}(s, a) \pi_{\boldsymbol\theta}(a \mid s)\nabla_{\boldsymbol\theta}\log{\pi_{\boldsymbol\theta}(a \mid s)}\nonumber\\
    &= \mathbb{E}_\pi[q_{\pi_{\boldsymbol\theta}}(s, a)\nabla_{\boldsymbol\theta}\log{\pi_{\boldsymbol\theta}(a \mid s)}] \label{eq:grad-log-prob}
\end{align}
where $\mathbb{E}_\pi$ is the expectation when $s \sim d^{\pi_{\boldsymbol\theta}}$ and $a \sim \pi_{\boldsymbol\theta}$, i.e. both state and action distributions follow policy $\pi_{\boldsymbol\theta}$. Expressing the gradient as an expectation means we can estimate it using a sample mean. We let the agent interact with the environment following a policy $\pi_\theta$ and collect its \textit{trajectory} $\tau_i = \{s_0, a_0, \dots, s_{T+1}\}$ over $N$ episodes, obtaining a set $\mathcal{D} = \{\tau_i\}_{i=1,\dots,N}$ of trajectories. Then, the policy gradient is estimated as:
\begin{equation}\label{pg_estimator}
    \hat{g} = \frac{1}{|\mathcal{D}|}\sum_{\tau \in \mathcal{D}} \sum_{t=0}^{T} q_{\pi_{\boldsymbol\theta}}(s_t, a_t)\nabla_{\boldsymbol\theta}\log{\pi_{\boldsymbol\theta}(a_t \mid s_t)}
\end{equation}
That is, we compute the expression inside the expectation in each episode and take the sample mean as an estimator for the gradient, allowing us to take an update step.

The policy gradient, while elegant, has shown to be problematic in practical problems:

\textbf{Sample inefficiency.} In order to run policy gradient, we need to sample from our policy millions and millions of times, since the estimation is done using Monte Carlo, averaging over a number of trial runs. Summing over all steps in a single trajectory could be very expensive computationally depending on the environment. It is also worth noting sample inefficiency is not a problem exclusive to policy gradient, it is an issue that has long plagued a lot of other RL algorithms.

\textbf{Slow convergence.} Sampling millions of trajectories is already inherently slow, and the high variance makes optimization very inefficient.

\textbf{High variance.} The high variance comes from the fact that, in RL, we are often dealing with very general problems. In our case, teaching a car to navigate through a parking lot. When we sample from an untrained policy, we are bound to observe highly variable behaviors, since we begin with a policy whose distribution of actions over states is effectively uniform. Of course, as the policy improves, the distribution is shaped to be unimodal or multimodal over some successful actions given a state. But in order to get there, we need the model to observe the outcomes of many different actions for each possible state. If we consider the action and state spaces to be continuous, the problem is even worse, since visiting every action-state pair possible is computationally intractable.

Equation \ref{eq:grad-log-prob}, also known as \textit{grad-log-prob}, gives rise to an important result, which was used to derive a lot of other methods as an improvement over the \textit{vanilla} policy gradient.
\begin{lemma}{Expected Grad-Log-Prob (EGLP)}{eglp}
    Suppose that $P_\theta$ is a parameterized probability distribution over a random variable $x$. Then:
    \begin{equation}
        \mathop{\mathbb{E}}_{x \sim P_\theta}\left[\nabla_\theta \log{P_\theta(x)}\right] = 0
    \end{equation}
\end{lemma}
\begin{proof}
    First, recall that probability distributions are normalized:
    \begin{equation}
        \int_x P_\theta(x) \,dx = 1
    \end{equation}
    By taking the gradient on both sides, we get:
    \begin{equation}
        \nabla_\theta\int_x P_\theta(x) \,dx = \nabla_\theta 1 = 0
    \end{equation}
    Now, we can use the log derivative trick:
    \begin{align}
        \nabla_\theta\int_x P_\theta(x) \,dx &= 0 \nonumber\\
        \int_x \nabla_\theta P_\theta(x) \,dx &= 0 \\
        \int_x P_\theta(x) \nabla_\theta\log{P_\theta(x)} \,dx &= 0 \nonumber
    \end{align}
\end{proof}

As an consequence of the above lemma, \cite{https://doi.org/10.48550/arxiv.1506.02438} proposed a more general form for policy gradients:
\begin{equation}
    \nabla_{\boldsymbol\theta} J(\boldsymbol\theta) = \mathbb{E}_\pi\left[\sum_{t=0}^\infty\Phi_t\nabla_{\boldsymbol\theta}\log{\pi_{\boldsymbol\theta}(a \mid s)}\right]
\end{equation}
where $\Phi_t$ may be any of these functions:
\begin{itemize}
    \item $\sum_{t=0}^{\infty} r_t:$ sum of total rewards.
    \item $\sum_{t^{\prime}=t}^{\infty} r_{t^{\prime}}$ : reward following action $a_t$.
    \item $q_\pi\left(s_t, a_t\right)$ : state-action value function.
    \item $A_\pi\left(s_t, a_t\right)$ : advantage function.
\end{itemize}
\cite{https://doi.org/10.48550/arxiv.1506.02438} lists all the possible functions. For trust region methods and proximal policy optimization, $\Phi_t$ is chosen to be the advantage function $A_\pi\left(s_t, a_t\right)$.

The formulation of policy gradients with advantage functions is rather common, but we then need a way to estimate it. The most known method is \textit{generalized advantage estimation} as described in \cite{https://doi.org/10.48550/arxiv.1506.02438}.

\section{Trust Region Policy Optimization}
TRPO updates policies by taking the largest step possible to improve performance, while satisfying a special constraint on how close the new and old policies are allowed to be.

Normal policy gradient keeps new and old policies close in parameter space, but even small differences in parameter space can have a large impact in performance, such that a single bad step can collapse the policy performance. Thus, it is dangerous to use large step sizes with vanilla policy gradients, which ultimately makes the method very sample inefficient. TRPO not only avoids this kind of collapse, but also tends to quickly and monotonically improve performance.

The way TRPO achieves this is by guaranteeing that the policy doesn't change too much in comparison to the old one using Kullback-Leibler divergence. KL divergence is a statistical measure of how different a probability distribution is from another.

The objective function in TRPO is
\begin{equation}
    J(\theta) = \displaystyle\mathop{\mathbb{E}}_{a \sim \pi_{\theta_\text{old}}}\left[\frac{\pi_{\theta}(a \mid s)}{\pi_{\theta_\text{old}}(a \mid s)}\hat{A}_{\pi_{\theta_\text{old}}}\right]
\end{equation}
and the goal is to maximize it subject to the \textit{trust region constraint}, which enforces the distance between old and new policies, measured by KL-divergence, to be small enough:
\begin{equation}
    \mathbb{E}[\text{KL}(\pi_{\theta_\text{old}}(a \mid s), \pi_\theta(a \mid s))] \leq \delta
\end{equation}
That way, the old and new policies would not diverge too much when this hard constraint is met. Not only that, but TRPO also guarantees monotonic improvement over each iteration.

The full details of the derivation of this method have been omitted, but can be found in \cite{DBLP:journals/corr/SchulmanLMJA15}. While TRPO is not the focus of this work, it was one of the motivators of \textit{proximal policy optimization}.

The same way TRPO emerged as an improvement over vanilla policy gradient, PPO emerges as an improvement over TRPO. In short, some of the major disadvantages of TRPO is that it is computationally expensive and still sample inefficient.

\section{Proximal Policy Optimization}
Proximal policy optimization was proposed in \cite{schulman2017proximal}
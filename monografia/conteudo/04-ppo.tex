\chapter{Proximal Policy Optimization}
Proximal policy optimization (PPO) was proposed by \cite{https://doi.org/10.48550/arxiv.1707.06347} as an alternative to already existing policy gradient (PG) methods, incorporating some concepts from trust region policy optimization (TRPO) methods, retaining some of its benefits while being significantly easier to implement.
\section{Policy Gradient}
For policy gradient, we consider parameterized policies, which can select actions without relying on a value function. The value function is still useful to learn the policy parameters, but it's not strictly necessary to select an action. This parameterization can be done in any way as long as the policy is differentiable with respect to its parameters.

Denoting by $\boldsymbol\theta \in \mathbb{R}^d$ the policy parameter vector, the probability of selecting action $a$ at time $t$ given that the environment is in state $s$ with parameter $\boldsymbol\theta$ is
\[
    \pi_{\boldsymbol\theta}(a \mid s, \boldsymbol\theta) = P\{A_t = a \mid S_t = s, \boldsymbol\theta_t = \boldsymbol\theta\}   
\]
We also need to define a performance measure $J(\boldsymbol\theta)$ to quantify how good a policy is. Policy gradient algorithms search for a local maximum in $J$ using gradient ascent:
\[
    \Delta\boldsymbol\theta = \alpha\nabla_{\boldsymbol\theta}J(\boldsymbol\theta)   
\]
where $\nabla_{\boldsymbol\theta}J(\boldsymbol\theta)$ is the policy gradient defined as
\begin{align*}
    \nabla_{\boldsymbol\theta}J(\boldsymbol\theta) &=
    \begin{bmatrix}
        \frac{\partial J(\boldsymbol\theta)}{\partial \theta_1} \\
        \vdots \\
        \frac{\partial J(\boldsymbol\theta)}{\partial \theta_n}
    \end{bmatrix}
\end{align*} 
and $\alpha$ is a step size parameter, commonly called \textit{learning rate}.    

In our case, we define such measure to be $J(\boldsymbol\theta) = v_{\pi{\boldsymbol\theta}}(s_0)$, where $s_0$ denotes the starting state and $v_{\pi_{\boldsymbol\theta}}$ is the true value function for the parameterized policy $\pi_{\boldsymbol\theta}$. That is, the measure is the expected sum of discounted rewards.

For that particular choice of $J(\boldsymbol\theta)$, we have a convenient way of expressing its gradient using the Policy Gradient Theorem (adapted from \cite{NIPS1999_464d828b}):
\begin{theorem}{Policy Gradient Theorem}{pg_theorem}
    Given an MDP $\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$ and a parameterized policy $\pi_{\boldsymbol\theta}$, the gradient of the expected return $J(\boldsymbol\theta)$ is given by
    \begin{equation}
        \nabla_{\boldsymbol\theta} J(\boldsymbol\theta) \propto \sum_{s \in \mathcal{S}} d^{\pi_{\boldsymbol\theta}}(s) \sum_{a \in \mathcal{A}} q_{\pi_{\boldsymbol\theta}}(s, a) \nabla_{\boldsymbol\theta}\pi_{\boldsymbol\theta}(a \mid s)
    \end{equation}
\end{theorem}
Where $d^{\pi_{\boldsymbol\theta}}(s) = \lim_{t \to \infty} P\{s_t = s \mid s_0, \pi\}$ is the stationary distribution of states under policy $\pi$, which is assumed to be independent of the starting state $s_0$. Furthermore, we can use the following identity, commonly called the "log derivative trick", to rewrite the expression for the gradient:
\begin{align*}
    \nabla_{\boldsymbol\theta}\pi_{\boldsymbol\theta}(a \mid s) &= \pi_{\boldsymbol\theta}(a \mid s)\frac{\nabla_{\boldsymbol\theta}\pi_{\boldsymbol\theta}(a \mid s)}{\pi_{\boldsymbol\theta}(a \mid s)} \\ 
     &= \pi_{\boldsymbol\theta}(a \mid s)\nabla_{\boldsymbol\theta}\log{\pi_{\boldsymbol\theta}(a \mid s)}
\end{align*}
Then, using the identity above, we are able to express the gradient as an expectation:
\begin{align}
    \nabla_{\boldsymbol\theta} J(\boldsymbol\theta) &\propto \sum_{s \in \mathcal{S}} d^{\pi_{\boldsymbol\theta}}(s) \sum_{a \in \mathcal{A}} q_{\pi_{\boldsymbol\theta}}(s, a) \nabla_{\boldsymbol\theta}\pi_{\boldsymbol\theta}(a \mid s)\\
    &= \sum_{s \in \mathcal{S}} d^{\pi_{\boldsymbol\theta}}(s) \sum_{a \in \mathcal{A}} q_{\pi_{\boldsymbol\theta}}(s, a) \nabla_{\boldsymbol\theta}\pi_{\boldsymbol\theta}(a \mid s)\frac{\pi_{\boldsymbol\theta}(a \mid s)}{\pi_{\boldsymbol\theta}(a \mid s)}\nonumber\\
    &= \sum_{s \in \mathcal{S}} d^{\pi_{\boldsymbol\theta}}(s) \sum_{a \in \mathcal{A}} q_{\pi_{\boldsymbol\theta}}(s, a) \pi_{\boldsymbol\theta}(a \mid s)\frac{\nabla_{\boldsymbol\theta}\pi_{\boldsymbol\theta}(a \mid s)}{\pi_{\boldsymbol\theta}(a \mid s)}\nonumber\\
    &= \sum_{s \in \mathcal{S}} d^{\pi_{\boldsymbol\theta}}(s) \sum_{a \in \mathcal{A}} q_{\pi_{\boldsymbol\theta}}(s, a) \pi_{\boldsymbol\theta}(a \mid s)\nabla_{\boldsymbol\theta}\log{\pi_{\boldsymbol\theta}(a \mid s)}\nonumber\\
    &= \mathbb{E}_\pi[q_{\pi_{\boldsymbol\theta}}(s, a)\nabla_{\boldsymbol\theta}\log{\pi_{\boldsymbol\theta}(a \mid s)}] \label{eq:grad-log-prob}
\end{align}
where $\mathbb{E}_\pi$ is the expectation when $s \sim d^{\pi_{\boldsymbol\theta}}$ and $a \sim \pi_{\boldsymbol\theta}$, i.e. both state and action distributions follow policy $\pi_{\boldsymbol\theta}$. Equation \ref{eq:grad-log-prob} is also known as \textit{grad-log-prob} and gives rise to an important result which is extensively used throughout policy gradient theory.
\begin{lemma}{Expected Grad-Log-Prob (EGLP)}{eglp}
    Suppose that $P_\theta$ is a parameterized probability distribution over a random variable $x$. Then:
    \begin{equation}
        \mathop{\mathbb{E}}_{x \sim P_\theta}\left[\nabla_\theta \log{P_\theta(x)}\right] = 0
    \end{equation}
\end{lemma}
\begin{proof}
    First, recall that probability distributions are normalized:
    \begin{equation}
        \int_x P_\theta(x) \,dx = 1
    \end{equation}
    By taking the gradient on both sides, we get:
    \begin{equation}
        \nabla_\theta\int_x P_\theta(x) \,dx = \nabla_\theta 1 = 0
    \end{equation}
    Now, we can use the log derivative trick:
    \begin{align}
        \nabla_\theta\int_x P_\theta(x) \,dx &= 0 \nonumber\\
        \int_x \nabla_\theta P_\theta(x) \,dx &= 0 \\
        \int_x P_\theta(x) \nabla_\theta\log{P_\theta(x)} \,dx &= 0 \nonumber
    \end{align}
\end{proof}

The policy gradient theorem lays the foundation for many policy gradient algorithms. The \textit{vanilla policy gradient} update proposed above is unbiased, but has high variance.

As an consequence of the above lemma, \cite{https://doi.org/10.48550/arxiv.1506.02438} proposed a more general form for policy gradients:
\begin{equation}
    \nabla_{\boldsymbol\theta} J(\boldsymbol\theta) = \mathbb{E}_\pi\left[\sum_{t=0}^\infty\Psi_t\nabla_{\boldsymbol\theta}\log{\pi_{\boldsymbol\theta}(a \mid s)}\right]
\end{equation}

\section{Trust Region Methods}
TRPO updates policies by taking the largest step possible to improve performance, while satisfying a special constraint on how close the new and old policies are allowed to be. The constraint is expressed in terms of KL-Divergence, a measure of (something like, but not exactly) distance between probability distributions.

This is different from normal policy gradient, which keeps new and old policies close in parameter space. But even seemingly small differences in parameter space can have very large differences in performanceâ€”so a single bad step can collapse the policy performance. This makes it dangerous to use large step sizes with vanilla policy gradients, thus hurting its sample efficiency. TRPO nicely avoids this kind of collapse, and tends to quickly and monotonically improve performance.

